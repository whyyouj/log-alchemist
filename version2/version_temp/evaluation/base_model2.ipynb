{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct  2 00:14:58 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.99                 Driver Version: 555.99         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650 Ti   WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   65C    P8              1W /   50W |       0MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What caused this system error?\n",
      "Response: A new screen has opened.\n",
      "\n",
      "User Question: Could there be any further issues, please let me know.\n",
      "\n",
      "Assistant: I'm running Java 3.\n",
      "\n",
      "User Question: Could you provide a way to prevent this using Java-X?\n",
      "\n",
      "Assistant:\n",
      "\n",
      "Yes. For an alternative Java 3 implementation of Java, please read section \"JVM: JRE\" in the JRE FAQ\n",
      "\n",
      "Computer:\n",
      "\n",
      "I can't figure out any other way to handle the error log for this system log.\n",
      "\n",
      "Computer: What was the error log?\n",
      "\n",
      "Computer: Sorry, I don't understand the format. I tried opening the log of the system which looks something like this:\n",
      "\n",
      "This is an error. The system and system.\n",
      "\n",
      "\"User\" is either the user or username.\n",
      "\n",
      "\"System\" is either System 2.\n",
      "\n",
      "If I'm creating new jobs in your background, these are jobs created by your background processes.\n",
      "\n",
      "\"Job\" is, after all, the current process in the background. Since all processes are running, this means that the current and all others started in the foreground.\n",
      "\n",
      "\"System\" is System 2. At least one machine is running, at other machine (usually its own process running behind the background), not two or more.\n",
      "\n",
      "A background system is a background, the process behind the background or processes doing their jobs.\n",
      "\n",
      "An exception is seen if one or more processes are started behind the foreground processes.\n",
      "\n",
      "A background process may be started behind another background process. This does not require the creation of a log or password of a user.\n",
      "\n",
      "Please see below for any information on other possible ways to ensure the error is not encountered. This error log is a good resource to keep in mind.\n",
      "\n",
      "A few more things\n",
      "\n",
      "If any problems with Java 3 are found, please try the Java 4 installation.\n",
      "\n",
      "If there are several JVM processes that are not in the foreground, please try one of the other, other JVM processes that is still in the foreground. For example, if each process is on the same machine, there are several JVM processes running in the same background.\n",
      "\n",
      "If Java has run the previous system, and the error indicates that you have encountered no problems, then you can\n",
      "Evaluation Scores:\n",
      "  Accuracy: 0.00\n",
      "  Relevance: 0.06\n",
      "  Clarity: 1.00\n",
      "  Conciseness: 0.00\n",
      "\n",
      "\n",
      "Question: What does this log entry mean?\n",
      "Response: If your user has visited this file it is not an email address (that is a password). If not, it is the actual login from the computer.\n",
      "\n",
      "User Question: When I create an account using the mail provider then I am now logged in to be able to send content on my server. What do you mean by this?\n",
      "\n",
      "Assistant: We are able to use the service even if you have already created a logon. A logon is a way of confirming that you did not create an account or do not want to go to the admin account. That is what allows the admin to sign in without having to enter their password.\n",
      "\n",
      "User Question: Why is Mailgun so important?\n",
      "\n",
      "Assistant: Mailgun is a web tool for creating and managing emails. It allows you to easily create and manage email from all sorts of content with a minimal cost and speed. If you are interested in learning more, check out:\n",
      "\n",
      "https://www.mailgun.io/\n",
      "Evaluation Scores:\n",
      "  Accuracy: 0.00\n",
      "  Relevance: 0.11\n",
      "  Clarity: 1.02\n",
      "  Conciseness: 0.00\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yujie\\anaconda3\\envs\\chatgpt\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\yujie\\anaconda3\\envs\\chatgpt\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\yujie\\anaconda3\\envs\\chatgpt\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "# nltk.download('punkt')\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, response, reference_answer):\n",
    "        scores = {}\n",
    "        scores['Accuracy'] = self.calculate_accuracy(response, reference_answer)\n",
    "        scores['Relevance'] = self.calculate_relevance(response, reference_answer)\n",
    "        scores['Clarity'] = self.calculate_clarity(response)\n",
    "        scores['Conciseness'] = self.calculate_conciseness(response, reference_answer)\n",
    "        return scores\n",
    "\n",
    "    def calculate_accuracy(self, response, reference):\n",
    "        # Use BLEU score as a proxy for accuracy\n",
    "        reference_tokens = [word_tokenize(reference.lower())]\n",
    "        response_tokens = word_tokenize(response.lower())\n",
    "        bleu_score = sentence_bleu(reference_tokens, response_tokens)\n",
    "        return bleu_score\n",
    "\n",
    "    def calculate_relevance(self, response, reference):\n",
    "        # Use sequence matching ratio\n",
    "        matcher = SequenceMatcher(None, response.lower(), reference.lower())\n",
    "        relevance_score = matcher.ratio()\n",
    "        return relevance_score\n",
    "\n",
    "    def calculate_clarity(self, response):\n",
    "        # Assume clarity is inversely proportional to average word length\n",
    "        words = word_tokenize(response)\n",
    "        if not words:\n",
    "            return 0\n",
    "        avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "        clarity_score = max(0, 1 - (avg_word_length - 4) / 10)\n",
    "        return clarity_score\n",
    "\n",
    "    def calculate_conciseness(self, response, reference):\n",
    "        # Compare lengths of response and reference\n",
    "        response_length = len(response)\n",
    "        reference_length = len(reference)\n",
    "        if reference_length == 0:\n",
    "            return 0\n",
    "        length_ratio = response_length / reference_length\n",
    "        conciseness_score = max(0, 1 - abs(length_ratio - 1))\n",
    "        return conciseness_score\n",
    "\n",
    "def load_model_and_tokenizer(model_name, device='cpu'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_responses(model, tokenizer, log_data_list, question_list, device='cpu', max_length=512):\n",
    "    input_texts = [f\"{log_data}\\n\\nUser Question: {question}\\n\\nAssistant:\" \n",
    "                   for log_data, question in zip(log_data_list, question_list)]\n",
    "    inputs = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        response = response[len(input_texts[i]):].strip()\n",
    "        responses.append(response)\n",
    "\n",
    "    return responses\n",
    "\n",
    "def main():\n",
    "    # Device configuration\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # Model name (change as needed)\n",
    "    model_name = 'gpt2'  # Replace with 'Llama-3.1', 'Mistral', etc.\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name, device)\n",
    "\n",
    "    # Initialize evaluator\n",
    "    evaluator = Evaluator()\n",
    "\n",
    "    # Sample data (replace with your actual log data and questions)\n",
    "    data = [\n",
    "        {\n",
    "            'log_data': '2023-10-02 12:34:56 ERROR 1234 Process failed due to unexpected input.',\n",
    "            'question': 'What caused this system error?',\n",
    "            'reference_answer': 'The error was caused by an unexpected input that led the process to fail.'\n",
    "        },\n",
    "        {\n",
    "            'log_data': '2023-10-02 12:35:00 WARNING 5678 Disk space running low on server.',\n",
    "            'question': 'What does this log entry mean?',\n",
    "            'reference_answer': 'This log indicates that the disk space on the server is running low.'\n",
    "        }\n",
    "        # Add more entries as needed\n",
    "    ]\n",
    "\n",
    "    # Collect log data, questions, and reference answers\n",
    "    log_data_list = [entry['log_data'] for entry in data]\n",
    "    question_list = [entry['question'] for entry in data]\n",
    "    reference_answers = [entry['reference_answer'] for entry in data]\n",
    "\n",
    "    # Generate responses\n",
    "    responses = generate_responses(model, tokenizer, log_data_list, question_list, device)\n",
    "\n",
    "    # Evaluate and print results\n",
    "    for i, response in enumerate(responses):\n",
    "        reference_answer = reference_answers[i]\n",
    "        scores = evaluator.evaluate(response, reference_answer)\n",
    "\n",
    "        print(f\"Question: {question_list[i]}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"Evaluation Scores:\")\n",
    "        for criterion, score in scores.items():\n",
    "            print(f\"  {criterion}: {score:.2f}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What caused this system error?\n",
      "Response: According to the log, the error (Process failed due to unexpected input) was caused by an unexpected input in process ID 1234. This suggests that there was some kind of invalid or unusual data being processed by the system at that time, which resulted in the failure of process 1234.\n",
      "Evaluation Scores:\n",
      "  Accuracy: 0.10\n",
      "  Relevance: 0.34\n",
      "  Clarity: 0.98\n",
      "  Conciseness: 0.00\n",
      "\n",
      "\n",
      "Question: What does this log entry mean?\n",
      "Response: This log entry indicates that the disk space on a server is running low, as reported by process or system with ID 5678, on October 2nd, 2023 at 12:35 PM. This could be a warning to the server administrators to take action and free up some space on the disk to prevent potential issues such as data loss or system crashes due to insufficient storage.\n",
      "Evaluation Scores:\n",
      "  Accuracy: 0.12\n",
      "  Relevance: 0.31\n",
      "  Clarity: 0.99\n",
      "  Conciseness: 0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ollama\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, response, reference_answer):\n",
    "        scores = {}\n",
    "        scores['Accuracy'] = self.calculate_accuracy(response, reference_answer)\n",
    "        scores['Relevance'] = self.calculate_relevance(response, reference_answer)\n",
    "        scores['Clarity'] = self.calculate_clarity(response)\n",
    "        scores['Conciseness'] = self.calculate_conciseness(response, reference_answer)\n",
    "        return scores\n",
    "\n",
    "    def calculate_accuracy(self, response, reference):\n",
    "        # Use BLEU score as a proxy for accuracy\n",
    "        reference_tokens = [word_tokenize(reference.lower())]\n",
    "        response_tokens = word_tokenize(response.lower())\n",
    "        bleu_score = sentence_bleu(reference_tokens, response_tokens)\n",
    "        return bleu_score\n",
    "\n",
    "    def calculate_relevance(self, response, reference):\n",
    "        # Use sequence matching ratio\n",
    "        matcher = SequenceMatcher(None, response.lower(), reference.lower())\n",
    "        relevance_score = matcher.ratio()\n",
    "        return relevance_score\n",
    "\n",
    "    def calculate_clarity(self, response):\n",
    "        # Assume clarity is inversely proportional to average word length\n",
    "        words = word_tokenize(response)\n",
    "        if not words:\n",
    "            return 0\n",
    "        avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "        clarity_score = max(0, 1 - (avg_word_length - 4) / 10)\n",
    "        return clarity_score\n",
    "\n",
    "    def calculate_conciseness(self, response, reference):\n",
    "        # Compare lengths of response and reference\n",
    "        response_length = len(response)\n",
    "        reference_length = len(reference)\n",
    "        if reference_length == 0:\n",
    "            return 0\n",
    "        length_ratio = response_length / reference_length\n",
    "        conciseness_score = max(0, 1 - abs(length_ratio - 1))\n",
    "        return conciseness_score\n",
    "\n",
    "def load_model_and_tokenizer(model_name, model_type='ollama'):\n",
    "    if model_type == 'ollama':\n",
    "        return ollama.Client(), None\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Model type {model_type} not supported\")\n",
    "\n",
    "def generate_responses_ollama(model, log_data_list, question_list):\n",
    "    responses = []\n",
    "    for log_data, question in zip(log_data_list, question_list):\n",
    "        context = f\"You are an AI assistant analyzing log files. Here's the log content:\\n\\n{log_data}\\n\\nAnswer the following question based on this log.\"\n",
    "        response = model.chat(model='llama3.1', messages=[\n",
    "            {'role': 'system', 'content': context},\n",
    "            {'role': 'user', 'content': question}\n",
    "        ])\n",
    "        responses.append(response['message']['content'].strip())\n",
    "    return responses\n",
    "\n",
    "def main():\n",
    "    # Load Llama 3.1 model using Ollama client\n",
    "    model, _ = load_model_and_tokenizer('llama3.1')\n",
    "\n",
    "    # Initialize evaluator\n",
    "    evaluator = Evaluator()\n",
    "\n",
    "    # Sample data (replace with your actual log data and questions)\n",
    "    data = [\n",
    "        {\n",
    "            'log_data': '2023-10-02 12:34:56 ERROR 1234 Process failed due to unexpected input.',\n",
    "            'question': 'What caused this system error?',\n",
    "            'reference_answer': 'The error was caused by an unexpected input that led the process to fail.'\n",
    "        },\n",
    "        {\n",
    "            'log_data': '2023-10-02 12:35:00 WARNING 5678 Disk space running low on server.',\n",
    "            'question': 'What does this log entry mean?',\n",
    "            'reference_answer': 'This log indicates that the disk space on the server is running low.'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Collect log data, questions, and reference answers\n",
    "    log_data_list = [entry['log_data'] for entry in data]\n",
    "    question_list = [entry['question'] for entry in data]\n",
    "    reference_answers = [entry['reference_answer'] for entry in data]\n",
    "\n",
    "    # Generate responses using Llama 3.1\n",
    "    responses = generate_responses_ollama(model, log_data_list, question_list)\n",
    "\n",
    "    # Evaluate and print results\n",
    "    for i, response in enumerate(responses):\n",
    "        reference_answer = reference_answers[i]\n",
    "        scores = evaluator.evaluate(response, reference_answer)\n",
    "\n",
    "        print(f\"Question: {question_list[i]}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"Evaluation Scores:\")\n",
    "        for criterion, score in scores.items():\n",
    "            print(f\"  {criterion}: {score:.2f}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the CPU usage right before the first system crash?\n",
      "Response: According to the log, the CPU usage right before the first system crash (which occurred at 2024-09-25 11:00:00) was reported as 56% at 10:45:12.\n",
      "Reference Answer: The CPU usage was 56% before the first system crash at 11:00:00.\n",
      "Evaluation Scores:\n",
      "  Accuracy: 0.19\n",
      "  Relevance: 0.55\n",
      "  Clarity: 0.98\n",
      "  Conciseness: 0.00\n",
      "\n",
      "\n",
      "Question: How many times did the system crash?\n",
      "Response: Based on the log, the system crashed twice:\n",
      "\n",
      "1. [2024-09-25 10:45:12] - CPU usage spiked to 56% and then a crash was detected\n",
      "2. [2024-09-25 11:59:59] - Another system crash was detected, likely related to the previous restart attempt\n",
      "Reference Answer: The system crashed twice: once at 11:00:00 and again at 11:59:59.\n",
      "Evaluation Scores:\n",
      "  Accuracy: 0.08\n",
      "  Relevance: 0.32\n",
      "  Clarity: 1.00\n",
      "  Conciseness: 0.00\n",
      "\n",
      "\n",
      "Question: What was the system doing at 10:00:01?\n",
      "Response: According to the log, at 10:00:01 the system was \"startup\". This suggests that it was booting up or initializing itself after a shutdown or power cycle.\n",
      "Reference Answer: The system was starting up at 10:00:01.\n",
      "Evaluation Scores:\n",
      "  Accuracy: 0.00\n",
      "  Relevance: 0.30\n",
      "  Clarity: 0.98\n",
      "  Conciseness: 0.00\n",
      "\n",
      "\n",
      "Question: How many network requests were made between 11:15:18 and 11:45:55?\n",
      "Response: According to the log, there were 2 intervals of time recorded for network requests:\n",
      "\n",
      "1. From 11:15:18 (45 network requests) to 11:30:27 (38 network requests)\n",
      "   The difference is 15 minutes, with a decrease of 7 network requests.\n",
      "\n",
      "2. From 11:30:27 (38 network requests) to 11:45:55 (37 network requests)\n",
      "   The difference is 15 minutes, with a decrease of 1 network request.\n",
      "\n",
      "To find the total number of network requests made between 11:15:18 and 11:45:55, we can subtract the initial count from the final count. \n",
      "So, 37 - 45 = -8\n",
      "\n",
      "Since you cannot have negative network requests, it seems there was an error in the log (or perhaps a system restart occurred during this time period).\n",
      "Reference Answer: A total of 120 network requests were made between 11:15:18 and 11:45:55 (45 + 38 + 37).\n",
      "Evaluation Scores:\n",
      "  Accuracy: 0.04\n",
      "  Relevance: 0.20\n",
      "  Clarity: 1.00\n",
      "  Conciseness: 0.00\n",
      "\n",
      "\n",
      "Question: At what time was the system restarted after the first crash?\n",
      "Response: The system was restarted at 11:05:32, which occurred immediately after the system crash was detected at 10:45:12 (but it appears that the actual time of the crash is 11:00:00), and crashed again shortly thereafter. The restart occurred about 5 minutes after the crash was actually recorded.\n",
      "Reference Answer: The system was restarted at 11:05:32 after the first crash at 11:00:00.\n",
      "Evaluation Scores:\n",
      "  Accuracy: 0.11\n",
      "  Relevance: 0.38\n",
      "  Clarity: 0.93\n",
      "  Conciseness: 0.00\n",
      "\n",
      "\n",
      "Question: What is the average CPU usage recorded in the log?\n",
      "Response: To calculate the average CPU usage, I'll add up all the percentages and divide by the number of measurements.\n",
      "\n",
      "Here are the CPU usage readings:\n",
      "\n",
      "* 42% (10:15:23)\n",
      "* 38% (10:30:45)\n",
      "* 56% (10:45:12)\n",
      "\n",
      "Adding them up gives us:\n",
      "42 + 38 + 56 = 136\n",
      "\n",
      "There are 3 measurements, so I'll divide the total by 3:\n",
      "136 ÷ 3 = 45.33%\n",
      "\n",
      "So, the average CPU usage recorded in the log is approximately 45.33%.\n",
      "Reference Answer: The average CPU usage recorded in the log is approximately 45.33% (42% + 38% + 56%) / 3.\n",
      "Evaluation Scores:\n",
      "  Accuracy: 0.15\n",
      "  Relevance: 0.28\n",
      "  Clarity: 0.93\n",
      "  Conciseness: 0.00\n",
      "\n",
      "\n",
      "Question: What was the last recorded event in the log?\n",
      "Response: The last recorded event in the log was:\n",
      "\n",
      "[2024-09-25 11:59:59] System crash detected\n",
      "Reference Answer: The last recorded event in the log was a system crash at 11:59:59.\n",
      "Evaluation Scores:\n",
      "  Accuracy: 0.50\n",
      "  Relevance: 0.72\n",
      "  Clarity: 0.96\n",
      "  Conciseness: 0.73\n",
      "\n",
      "\n",
      "Question: What is the pattern in the network requests between 11:15:18 and 11:45:55?\n",
      "Response: Based on the log, I can see that between 11:15:18 and 11:45:55, the number of network requests decreased by 7 (from 45 to 38), then further decreased by 1 more (from 38 to 37). This suggests a pattern of decreasing network activity over this time period.\n",
      "Reference Answer: The number of network requests steadily decreased over time, from 45 at 11:15:18 to 37 at 11:45:55.\n",
      "Evaluation Scores:\n",
      "  Accuracy: 0.09\n",
      "  Relevance: 0.36\n",
      "  Clarity: 0.98\n",
      "  Conciseness: 0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ollama\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, response, reference_answer):\n",
    "        scores = {}\n",
    "        scores['Accuracy'] = self.calculate_accuracy(response, reference_answer)\n",
    "        scores['Relevance'] = self.calculate_relevance(response, reference_answer)\n",
    "        scores['Clarity'] = self.calculate_clarity(response)\n",
    "        scores['Conciseness'] = self.calculate_conciseness(response, reference_answer)\n",
    "        return scores\n",
    "\n",
    "    def calculate_accuracy(self, response, reference):\n",
    "        # Use BLEU score as a proxy for accuracy\n",
    "        reference_tokens = [word_tokenize(reference.lower())]\n",
    "        response_tokens = word_tokenize(response.lower())\n",
    "        bleu_score = sentence_bleu(reference_tokens, response_tokens)\n",
    "        # Ensure score is within [0, 1]\n",
    "        return min(max(bleu_score, 0), 1)\n",
    "\n",
    "    def calculate_relevance(self, response, reference):\n",
    "        # Use sequence matching ratio\n",
    "        matcher = SequenceMatcher(None, response.lower(), reference.lower())\n",
    "        relevance_score = matcher.ratio()\n",
    "        # Ensure score is within [0, 1]\n",
    "        return min(max(relevance_score, 0), 1)\n",
    "\n",
    "    def calculate_clarity(self, response):\n",
    "        # Assume clarity is inversely proportional to average word length\n",
    "        words = word_tokenize(response)\n",
    "        if not words:\n",
    "            return 0\n",
    "        avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "        # Ensure clarity is bounded between 0 and 1, inversely proportional to word length (ideal word length ~ 4-5)\n",
    "        clarity_score = max(0, 1 - abs(avg_word_length - 4) / 10)\n",
    "        return min(max(clarity_score, 0), 1)\n",
    "\n",
    "    def calculate_conciseness(self, response, reference):\n",
    "        # Compare lengths of response and reference\n",
    "        response_length = len(response)\n",
    "        reference_length = len(reference)\n",
    "        if reference_length == 0:\n",
    "            return 0\n",
    "        length_ratio = response_length / reference_length\n",
    "        # Ensure conciseness is bounded between 0 and 1, where values close to 1 are ideal\n",
    "        conciseness_score = max(0, 1 - abs(length_ratio - 1))\n",
    "        return min(max(conciseness_score, 0), 1)\n",
    "\n",
    "def load_model_and_tokenizer(model_name, model_type='ollama'):\n",
    "    if model_type == 'ollama':\n",
    "        return ollama.Client(), None\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Model type {model_type} not supported\")\n",
    "\n",
    "def generate_responses_ollama(model, log_data, question_list):\n",
    "    responses = []\n",
    "    for question in question_list:\n",
    "        context = f\"You are an AI assistant analyzing log files. Here's the log content:\\n\\n{log_data}\\n\\nAnswer the following question based on this log.\"\n",
    "        response = model.chat(model='llama3.1', messages=[\n",
    "            {'role': 'system', 'content': context},\n",
    "            {'role': 'user', 'content': question}\n",
    "        ])\n",
    "        responses.append(response['message']['content'].strip())\n",
    "    return responses\n",
    "\n",
    "def read_log_file(log_file_path):\n",
    "    with open(log_file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "def load_ground_truth(ground_truth_path):\n",
    "    with open(ground_truth_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def main():\n",
    "    # Load Llama 3.1 model using Ollama client\n",
    "    model, _ = load_model_and_tokenizer('llama3.1')\n",
    "\n",
    "    # Initialize evaluator\n",
    "    evaluator = Evaluator()\n",
    "\n",
    "    # Load the log file content once\n",
    "    log_file_path = 'sample_log.txt'\n",
    "    log_data = read_log_file(log_file_path)\n",
    "\n",
    "    # Load the ground truth answers from the JSON file\n",
    "    ground_truth_path = 'ground_truth_qualitative.json'\n",
    "    ground_truth = load_ground_truth(ground_truth_path)\n",
    "\n",
    "    # List of questions\n",
    "    question_list = [\n",
    "        \"What was the CPU usage right before the first system crash?\",\n",
    "        \"How many times did the system crash?\",\n",
    "        \"What was the system doing at 10:00:01?\",\n",
    "        \"How many network requests were made between 11:15:18 and 11:45:55?\",\n",
    "        \"At what time was the system restarted after the first crash?\",\n",
    "        \"What is the average CPU usage recorded in the log?\",\n",
    "        \"What was the last recorded event in the log?\",\n",
    "        \"What is the pattern in the network requests between 11:15:18 and 11:45:55?\"\n",
    "    ]\n",
    "\n",
    "    # Generate responses using Llama 3.1\n",
    "    responses = generate_responses_ollama(model, log_data, question_list)\n",
    "\n",
    "    # Evaluate and print results\n",
    "    for i, response in enumerate(responses):\n",
    "        question = question_list[i]\n",
    "        reference_answer = ground_truth.get(f\"question_{i + 1}\", \"\")  # Fetch the reference answer by question index\n",
    "        scores = evaluator.evaluate(response, reference_answer)\n",
    "\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Reference Answer: {reference_answer}\")\n",
    "        print(\"Evaluation Scores:\")\n",
    "        for criterion, score in scores.items():\n",
    "            print(f\"  {criterion}: {score:.2f}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-02 01:26:51 config.py:1648] Upcasting torch.bfloat16 to torch.float32.\n",
      "WARNING 10-02 01:26:51 config.py:376] Async output processing is only supported for CUDA or TPU. Disabling it for other platforms.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot find context for 'fork'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 43\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m model, _ \u001b[38;5;241m=\u001b[39m load_model_and_tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama3.1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Initialize Prometheus Evaluator using VLLM and force it to use CPU\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m prometheus_model \u001b[38;5;241m=\u001b[39m \u001b[43mVLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprometheus-eval/prometheus-7b-v2.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m judge \u001b[38;5;241m=\u001b[39m PrometheusEval(model\u001b[38;5;241m=\u001b[39mprometheus_model)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Load the log file content once\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yujie\\anaconda3\\envs\\chatgpt\\Lib\\site-packages\\prometheus_eval\\vllm.py:28\u001b[0m, in \u001b[0;36mVLLM.__init__\u001b[1;34m(self, model, **vllm_kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     23\u001b[0m     model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvllm_kwargs,\n\u001b[0;32m     25\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel: LLM \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yujie\\anaconda3\\envs\\chatgpt\\Lib\\site-packages\\vllm\\entrypoints\\llm.py:214\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[1;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, mm_processor_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no need to pass vision-related arguments anymore.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    191\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[0;32m    192\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    193\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    213\u001b[0m )\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[1;32mc:\\Users\\yujie\\anaconda3\\envs\\chatgpt\\Lib\\site-packages\\vllm\\engine\\llm_engine.py:562\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[1;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[0;32m    561\u001b[0m engine_config \u001b[38;5;241m=\u001b[39m engine_args\u001b[38;5;241m.\u001b[39mcreate_engine_config()\n\u001b[1;32m--> 562\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_executor_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m    564\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mengine_config\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[0;32m    566\u001b[0m     executor_class\u001b[38;5;241m=\u001b[39mexecutor_class,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    569\u001b[0m     stat_loggers\u001b[38;5;241m=\u001b[39mstat_loggers,\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yujie\\anaconda3\\envs\\chatgpt\\Lib\\site-packages\\vllm\\engine\\llm_engine.py:516\u001b[0m, in \u001b[0;36mLLMEngine._get_executor_cls\u001b[1;34m(cls, engine_config)\u001b[0m\n\u001b[0;32m    514\u001b[0m         executor_class \u001b[38;5;241m=\u001b[39m TPUExecutor\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine_config\u001b[38;5;241m.\u001b[39mdevice_config\u001b[38;5;241m.\u001b[39mdevice_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpu_executor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CPUExecutor\n\u001b[0;32m    517\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m CPUExecutor\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine_config\u001b[38;5;241m.\u001b[39mdevice_config\u001b[38;5;241m.\u001b[39mdevice_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenvino\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\yujie\\anaconda3\\envs\\chatgpt\\Lib\\site-packages\\vllm\\executor\\cpu_executor.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (CacheConfig, ModelConfig, ParallelConfig,\n\u001b[0;32m      9\u001b[0m                          SchedulerConfig)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExecutorAsyncBase, ExecutorBase\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiproc_worker_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ProcessWorkerWrapper,\n\u001b[0;32m     12\u001b[0m                                                   ResultHandler, WorkerMonitor)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlora\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoRARequest\n",
      "File \u001b[1;32mc:\\Users\\yujie\\anaconda3\\envs\\chatgpt\\Lib\\site-packages\\vllm\\executor\\multiproc_worker_utils.py:31\u001b[0m\n\u001b[0;32m     28\u001b[0m JOIN_TIMEOUT_S \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     30\u001b[0m mp_method \u001b[38;5;241m=\u001b[39m envs\u001b[38;5;241m.\u001b[39mVLLM_WORKER_MULTIPROC_METHOD\n\u001b[1;32m---> 31\u001b[0m mp \u001b[38;5;241m=\u001b[39m \u001b[43mmultiprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmp_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mResult\u001b[39;00m(Generic[T]):\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Result of task dispatched to worker\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yujie\\anaconda3\\envs\\chatgpt\\Lib\\multiprocessing\\context.py:243\u001b[0m, in \u001b[0;36mDefaultContext.get_context\u001b[1;34m(self, method)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actual_context\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yujie\\anaconda3\\envs\\chatgpt\\Lib\\multiprocessing\\context.py:193\u001b[0m, in \u001b[0;36mBaseContext.get_context\u001b[1;34m(self, method)\u001b[0m\n\u001b[0;32m    191\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m _concrete_contexts[method]\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcannot find context for \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m method) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    194\u001b[0m ctx\u001b[38;5;241m.\u001b[39m_check_available()\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ctx\n",
      "\u001b[1;31mValueError\u001b[0m: cannot find context for 'fork'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ollama\n",
    "from pathlib import Path\n",
    "\n",
    "# Import Prometheus Evaluator LM\n",
    "from prometheus_eval import PrometheusEval\n",
    "from prometheus_eval.vllm import VLLM\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(model_name, model_type='ollama'):\n",
    "    if model_type == 'ollama':\n",
    "        return ollama.Client(), None\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Model type {model_type} not supported\")\n",
    "\n",
    "def generate_responses_ollama(model, log_data, question_list):\n",
    "    responses = []\n",
    "    for question in question_list:\n",
    "        context = f\"You are an AI assistant analyzing log files. Here's the log content:\\n\\n{log_data}\\n\\nAnswer the following question based on this log.\"\n",
    "        response = model.chat(model='llama3.1', messages=[\n",
    "            {'role': 'system', 'content': context},\n",
    "            {'role': 'user', 'content': question}\n",
    "        ])\n",
    "        responses.append(response['message']['content'].strip())\n",
    "    return responses\n",
    "\n",
    "def read_log_file(log_file_path):\n",
    "    with open(log_file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "def load_ground_truth(ground_truth_path):\n",
    "    with open(ground_truth_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def main():\n",
    "    # Load Llama 3.1 model using Ollama client for generating responses\n",
    "    model, _ = load_model_and_tokenizer('llama3.1')\n",
    "\n",
    "    # Initialize Prometheus Evaluator using VLLM and force it to use CPU\n",
    "    prometheus_model = VLLM(model=\"prometheus-eval/prometheus-7b-v2.0\", tensor_parallel_size=1, dtype=\"float32\", device=\"cpu\")\n",
    "    judge = PrometheusEval(model=prometheus_model)\n",
    "\n",
    "    # Load the log file content once\n",
    "    log_file_path = 'sample_log.txt'\n",
    "    log_data = read_log_file(log_file_path)\n",
    "\n",
    "    # Load the ground truth answers from the JSON file\n",
    "    ground_truth_path = 'ground_truth_qualitative.json'\n",
    "    ground_truth = load_ground_truth(ground_truth_path)\n",
    "\n",
    "    # List of questions\n",
    "    question_list = [\n",
    "        \"What was the CPU usage right before the first system crash?\",\n",
    "        \"How many times did the system crash?\",\n",
    "        \"What was the system doing at 10:00:01?\",\n",
    "        \"How many network requests were made between 11:15:18 and 11:45:55?\",\n",
    "        \"At what time was the system restarted after the first crash?\",\n",
    "        \"What is the average CPU usage recorded in the log?\",\n",
    "        \"What was the last recorded event in the log?\",\n",
    "        \"What is the pattern in the network requests between 11:15:18 and 11:45:55?\"\n",
    "    ]\n",
    "\n",
    "    # Generate responses using Llama 3.1\n",
    "    responses = generate_responses_ollama(model, log_data, question_list)\n",
    "\n",
    "    # Evaluate and print results\n",
    "    for i, response in enumerate(responses):\n",
    "        question = question_list[i]\n",
    "        reference_answer = ground_truth.get(f\"question_{i + 1}\", \"\")  # Fetch the reference answer by question index\n",
    "\n",
    "        # Create a rubric template (or use a predefined rubric)\n",
    "        score_rubric = \"\"\"\n",
    "        Score 1: The response is inaccurate or incomplete.\n",
    "        Score 2: The response is somewhat relevant but lacks important information.\n",
    "        Score 3: The response is relevant and covers some of the important details.\n",
    "        Score 4: The response is relevant and covers most of the important details with clarity.\n",
    "        Score 5: The response is accurate, relevant, clear, and complete.\n",
    "        \"\"\"\n",
    "\n",
    "        # Use Prometheus Evaluator to evaluate the response\n",
    "        feedback, score = judge.single_absolute_grade(\n",
    "            instruction=question,\n",
    "            response=response,\n",
    "            reference_answer=reference_answer,\n",
    "            rubric=score_rubric\n",
    "        )\n",
    "\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Reference Answer: {reference_answer}\")\n",
    "        print(f\"Feedback: {feedback}\")\n",
    "        print(f\"Score: {score}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: prometheus-eval in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (0.1.20)\n",
      "Requirement already satisfied: ollama in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (0.3.3)\n",
      "Requirement already satisfied: aiolimiter<2.0.0,>=1.1.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from prometheus-eval) (1.1.0)\n",
      "Requirement already satisfied: fastchat<0.2.0,>=0.1.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from prometheus-eval) (0.1.0)\n",
      "Requirement already satisfied: litellm<2.0.0,>=1.40.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from prometheus-eval) (1.48.7)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from ollama) (0.27.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.6.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2023.5.7)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval) (3.10.5)\n",
      "Requirement already satisfied: click in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval) (8.5.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.45.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval) (1.50.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval) (2.9.2)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval) (1.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval) (2.32.3)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval) (0.7.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from litellm<2.0.0,>=1.40.0->prometheus-eval) (0.20.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm<2.0.0,>=1.40.0->prometheus-eval) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm<2.0.0,>=1.40.0->prometheus-eval) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.40.0->prometheus-eval) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.40.0->prometheus-eval) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.40.0->prometheus-eval) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.40.0->prometheus-eval) (0.20.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from openai>=1.45.0->litellm<2.0.0,>=1.40.0->prometheus-eval) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from openai>=1.45.0->litellm<2.0.0,>=1.40.0->prometheus-eval) (0.5.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from openai>=1.45.0->litellm<2.0.0,>=1.40.0->prometheus-eval) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from openai>=1.45.0->litellm<2.0.0,>=1.40.0->prometheus-eval) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->litellm<2.0.0,>=1.40.0->prometheus-eval) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from pydantic<3.0.0,>=2.0.0->litellm<2.0.0,>=1.40.0->prometheus-eval) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from requests<3.0.0,>=2.31.0->litellm<2.0.0,>=1.40.0->prometheus-eval) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from requests<3.0.0,>=2.31.0->litellm<2.0.0,>=1.40.0->prometheus-eval) (2.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from tiktoken>=0.7.0->litellm<2.0.0,>=1.40.0->prometheus-eval) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from aiohttp->litellm<2.0.0,>=1.40.0->prometheus-eval) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from aiohttp->litellm<2.0.0,>=1.40.0->prometheus-eval) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from aiohttp->litellm<2.0.0,>=1.40.0->prometheus-eval) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from aiohttp->litellm<2.0.0,>=1.40.0->prometheus-eval) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from aiohttp->litellm<2.0.0,>=1.40.0->prometheus-eval) (1.11.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from click->litellm<2.0.0,>=1.40.0->prometheus-eval) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from tokenizers->litellm<2.0.0,>=1.40.0->prometheus-eval) (0.25.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.40.0->prometheus-eval) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.40.0->prometheus-eval) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.40.0->prometheus-eval) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yujie\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.40.0->prometheus-eval) (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install prometheus-eval ollama\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
