{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Evaluation of model with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "from graph_manager import Graph, global_graph\n",
    "\n",
    "class LanguageModelEvaluator:\n",
    "    def __init__(self):\n",
    "        if global_graph is None:\n",
    "            self.graph = Graph.create_graph()\n",
    "        else:\n",
    "            self.graph = global_graph\n",
    "\n",
    "    def generate_response(self, prompt: str) -> str:\n",
    "        response = self.graph.run(prompt)\n",
    "        if isinstance(response, bytes):\n",
    "            return response.decode('utf-8')\n",
    "        return str(response)\n",
    "\n",
    "    def extract_numeric_value(self, text: str) -> float:\n",
    "        if isinstance(text, bytes):\n",
    "            text = text.decode('utf-8')\n",
    "        matches = re.findall(r'\\d+(?:\\.\\d+)?', text)\n",
    "        if matches:\n",
    "            return float(matches[-1])\n",
    "        else:\n",
    "            raise ValueError(f\"No numeric value found in the text: {text}\")\n",
    "    \n",
    "    def calculate_accuracy(self, predicted: float, actual: float) -> float:\n",
    "        if predicted == actual:\n",
    "            return 100.0\n",
    "        elif actual == 0:\n",
    "            return 0.0 if predicted != 0 else 100.0\n",
    "        else:\n",
    "            error = abs(predicted - actual) / actual\n",
    "            accuracy = max(0, (1 - error)) * 100\n",
    "            return min(accuracy, 100.0)\n",
    "\n",
    "    def evaluate(self, ground_truth_file: str, prompts: List[str], metric_names: List[str]) -> Dict[str, Dict]:\n",
    "        with open(ground_truth_file, 'r') as f:\n",
    "            ground_truth = json.load(f)\n",
    "\n",
    "        results = {}\n",
    "        for prompt, metric_name in zip(prompts, metric_names):\n",
    "            model_response = self.generate_response(prompt)\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Model response: {model_response}\")\n",
    "            try:\n",
    "                extracted_value = self.extract_numeric_value(model_response)\n",
    "                ground_truth_value = ground_truth[metric_name]\n",
    "                print(f\"Extracted value: {extracted_value}\")\n",
    "                print(f\"Ground truth value: {ground_truth_value}\")\n",
    "                accuracy = self.calculate_accuracy(extracted_value, ground_truth_value)\n",
    "                results[metric_name] = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"model_response\": model_response,\n",
    "                    \"extracted_value\": extracted_value,\n",
    "                    \"ground_truth_value\": ground_truth_value,\n",
    "                    \"accuracy\": accuracy\n",
    "                }\n",
    "            except ValueError as e:\n",
    "                print(f\"Error processing {metric_name}: {str(e)}\")\n",
    "                results[metric_name] = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"model_response\": model_response,\n",
    "                    \"error\": str(e),\n",
    "                    \"accuracy\": 0.0\n",
    "                }\n",
    "            print(f\"Calculated accuracy: {results[metric_name]['accuracy']}%\\n\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_results(self, results: Dict[str, Dict], output_file: str = None):\n",
    "        if output_file is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_file = f\"./results/quantitative_evaluation_results_{timestamp}.json\"\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        \n",
    "        output = {\n",
    "            \"evaluation_time\": datetime.now().isoformat(),\n",
    "            \"results\": results\n",
    "        }\n",
    "\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(output, f, indent=2)\n",
    "\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "        return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How many times did the laptop crash in the past hour?\n",
      "Model response: According to the log, there were two system crashes detected within the last hour:\n",
      "\n",
      "1. [2024-09-25 10:00:01] to [2024-09-25 11:00:00]: No crashes detected\n",
      "2. [2024-09-25 11:00:00] System crash detected (first one)\n",
      "3. [2024-09-25 11:59:59] System crash detected (second one)\n",
      "\n",
      "Therefore, the laptop crashed **twice** within the last hour.\n",
      "Extracted value: 59.0\n",
      "Ground truth value: 2\n",
      "Calculated accuracy: 0%\n",
      "\n",
      "Prompt: What was the average CPU usage percentage?\n",
      "Model response: To calculate the average CPU usage, we need to add up all the CPU usage percentages and divide by the total number of readings.\n",
      "\n",
      "Here are the CPU usage readings:\n",
      "- 42%\n",
      "- 38%\n",
      "- 56%\n",
      "\n",
      "Total CPU usage: 42 + 38 + 56 = 136\n",
      "\n",
      "There were 3 CPU usage readings. To find the average, we divide the total by the number of readings:\n",
      "\n",
      "Average CPU usage: 136 / 3 = 45.33%\n",
      "\n",
      "So, the average CPU usage percentage was approximately 45.33%.\n",
      "Extracted value: 45.33\n",
      "Ground truth value: 45.3\n",
      "Calculated accuracy: 99.93377483443709%\n",
      "\n",
      "Prompt: How many total network requests were made?\n",
      "Model response: There are 3 instances of network request logs in the given log content:\n",
      "\n",
      "1. [2024-09-25 11:15:18] Network requests: 45\n",
      "2. [2024-09-25 11:30:27] Network requests: 38\n",
      "3. [2024-09-25 11:45:55] Network requests: 37\n",
      "\n",
      "To find the total number of network requests made, we need to add the values from each log:\n",
      "\n",
      "45 + 38 + 37 = 120\n",
      "\n",
      "So, there were a total of **120** network requests made.\n",
      "Extracted value: 120.0\n",
      "Ground truth value: 120\n",
      "Calculated accuracy: 100.0%\n",
      "\n",
      "Results saved to ./results/quantitative_evaluation_results_20241005_205414.json\n",
      "\n",
      "Evaluation Results:\n",
      "laptop_crashes: 0.00% accuracy\n",
      "avg_cpu_usage: 99.93% accuracy\n",
      "total_network_requests: 100.00% accuracy\n",
      "\n",
      "Detailed results have been saved to ./results/quantitative_evaluation_results_20241005_205414.json\n"
     ]
    }
   ],
   "source": [
    "# from language_model_evaluator import LanguageModelEvaluator\n",
    "def main():\n",
    "    model_name = \"llama3.1\" \n",
    "    log_file = \"sample_log.txt\"\n",
    "    ground_truth_file = \"ground_truth.json\"\n",
    "    prompts = [\n",
    "        \"How many times did the laptop crash in the past hour?\",\n",
    "        \"What was the average CPU usage percentage?\",\n",
    "        \"How many total network requests were made?\"\n",
    "    ]\n",
    "    metric_names = [\"laptop_crashes\", \"avg_cpu_usage\", \"total_network_requests\"]\n",
    "\n",
    "    evaluator = LanguageModelEvaluator(model_name)\n",
    "    results = evaluator.evaluate(log_file, ground_truth_file, prompts, metric_names)\n",
    "\n",
    "    output_file = evaluator.save_results(results)\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    for metric, data in results.items():\n",
    "        print(f\"{metric}: {data['accuracy']:.2f}% accuracy\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STAGE] Try again agent: No\n",
      "[STAGE] Router Agent\n",
      "ROUTER AGENT OUTPUT:  Thought: The user question is asking for a specific count of events in the dataframe, which can be achieved through data manipulation using Python and pandas.\n",
      "\n",
      "Answer: yes.\n",
      "[STAGE] Router summary agent\n",
      "ROUTER SUMMARY AGENT OUTPUT:  no. \n",
      "\n",
      "the question is asking for a specific count of occurrences, which implies it's looking for a detailed answer rather than a high-level overview.\n",
      "[STAGE] Pandas AI agent\n",
      "Prompt: How many times did the event with eventid E189 occur?\n",
      "Model response: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot use a string pattern on a bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 19\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m metric_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE189\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE188\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE120\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE203\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE323\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcom.apple.cts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorecaptured\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQQ\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMicrosoft Word\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthorMacBook-Pro\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     18\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m LanguageModelEvaluator()\n\u001b[1;32m---> 19\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric, result \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[25], line 46\u001b[0m, in \u001b[0;36mLanguageModelEvaluator.evaluate\u001b[1;34m(self, ground_truth_file, prompts, metric_names)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m     extracted_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_numeric_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     ground_truth_value \u001b[38;5;241m=\u001b[39m ground_truth[metric_name]\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextracted_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 20\u001b[0m, in \u001b[0;36mLanguageModelEvaluator.extract_numeric_value\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_numeric_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m---> 20\u001b[0m     matches \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md+(?:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md+)?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m matches:\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(matches[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\yujie\\anaconda3\\envs\\py310_cuda\\lib\\re.py:240\u001b[0m, in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    233\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m \n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot use a string pattern on a bytes-like object"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    ground_truth_file = \"mac_ground_truth.json\"\n",
    "    prompts = [\n",
    "        \"How many times did the event with eventid E189 occur?\",\n",
    "        \"How many times did the event with eventid E188 occur?\",\n",
    "        \"How many times did the event with eventid E120 occur?\",\n",
    "        \"How many times did the event with eventid E203 occur?\",\n",
    "        \"How many times did the event with eventid E323 occur?\",\n",
    "        \"How many times did the event with component kernel occur?\",\n",
    "        \"How many times did the event with component com.apple.cts occur?\",\n",
    "        \"How many times did the event with component corecaptured occur?\",\n",
    "        \"How many times did the event with component QQ occur?\",\n",
    "        \"How many times did the event with component Microsoft Word occur?\",\n",
    "        \"How many times did the event with the user authorMacBook-Pro occur?\",\n",
    "    ]\n",
    "    metric_names = ['E189', 'E188', 'E120', 'E203', 'E323', 'kernel', 'com.apple.cts', 'corecaptured', 'QQ', 'Microsoft Word', 'authorMacBook-Pro']\n",
    "\n",
    "    evaluator = LanguageModelEvaluator()\n",
    "    results = evaluator.evaluate(ground_truth_file, prompts, metric_names)\n",
    "\n",
    "    print(\"Evaluation Results:\")\n",
    "    for metric, result in results.items():\n",
    "        print(f\"{metric}: {result['accuracy']:.2f}% accuracy\")\n",
    "\n",
    "    evaluator.save_results(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the main function\n",
    "evaluation_results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_manager import Graph, global_graph\n",
    "# Creates global graph if not yet created\n",
    "if global_graph is None:\n",
    "    graph = Graph.create_graph()\n",
    "else:\n",
    "    graph = global_graph\n",
    "\n",
    "result = graph.run(\"You are amazing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from language_model_evaluator import LanguageModelEvaluator\n",
    "# from graph_manager import Graph, global_graph\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "def main():\n",
    "    model_name = \"pandasai\" \n",
    "    log_file = \"Mac_2k.log_structured.csv\"\n",
    "    ground_truth_file = \"mac_ground_truth.json\"\n",
    "    prompts = [\n",
    "        \"How many times did the event with eventid E189 occur?\",\n",
    "        \"How many times did the event with eventid E188 occur?\",\n",
    "        \"How many times did the event with eventid E120 occur?\",\n",
    "        \"How many times did the event with eventid E203 occur?\",\n",
    "        \"How many times did the event with eventid E323 occur?\",\n",
    "        \"How many times did the event with component kernel occur?\",\n",
    "        \"How many times did the event with component com.apple.cts occur?\",\n",
    "        \"How many times did the event with component corecaptured occur?\",\n",
    "        \"How many times did the event with component QQ occur?\",\n",
    "        \"How many times did the event with component Microsoft Word occur?\",\n",
    "        \"How many times did the event with the user authorMacBook-Pro occur?\",\n",
    "    ]\n",
    "    metric_names = ['E189', 'E188', 'E120', 'E203', 'E323', 'kernel', 'com.apple.cts', 'corecaptured', 'QQ', 'Microsoft Word', 'authorMacBook-Pro']\n",
    "\n",
    "    evaluator = LanguageModelEvaluator(model_name, model_type='pandasai')\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(log_file)\n",
    "    evaluator.set_pandas_agent(df)\n",
    "    \n",
    "    results = evaluator.evaluate(log_file, ground_truth_file, prompts, metric_names)\n",
    "\n",
    "    print(\"Evaluation Results:\")\n",
    "    for metric, result in results.items():\n",
    "        print(f\"{metric}: {result['accuracy']:.2f}% accuracy\")\n",
    "\n",
    "    # Graph setup\n",
    "    global global_graph\n",
    "    if global_graph is None:\n",
    "        global_graph = Graph.create_graph()\n",
    "\n",
    "    # # Use the graph object\n",
    "    # graph_result = global_graph.run(\"You are amazing!\")\n",
    "    # print(\"Graph result:\", graph_result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main function\n",
    "# evaluation_results, graph_output = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
