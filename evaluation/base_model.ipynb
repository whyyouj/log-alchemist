{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Evaluation of model with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import ollama\n",
    "import csv\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "class LanguageModelEvaluator:\n",
    "    def __init__(self, model_name: str, model_type: str = 'ollama'):\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.model = self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        if self.model_type == 'ollama':\n",
    "            return ollama.Client()\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Model type {self.model_type} not supported\")\n",
    "\n",
    "    def generate_response(self, prompt: str, context: str) -> str:\n",
    "        if self.model_type == 'ollama':\n",
    "            response = self.model.chat(model=self.model_name, messages=[\n",
    "                {\n",
    "                    'role': 'system',\n",
    "                    'content': f\"You are an AI assistant analyzing log files. Here's the log content:\\n\\n{context}\\n\\nAnswer the following question based on this log and remember to return a number at the end where applicable:\"\n",
    "                },\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': prompt,\n",
    "                }\n",
    "            ])\n",
    "            return response['message']['content']\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Model type {self.model_type} not supported\")\n",
    "\n",
    "    def extract_numeric_value(self, text: str) -> float:\n",
    "        # Use a non-capturing group to get full matches\n",
    "        matches = re.findall(r'\\d+(?:\\.\\d+)?', text)\n",
    "        if matches:\n",
    "            # Return the last numeric value found\n",
    "            return float(matches[-1])\n",
    "        else:\n",
    "            raise ValueError(f\"No numeric value found in the text: {text}\")\n",
    "    \n",
    "    def calculate_accuracy(self, predicted: float, actual: float) -> float:\n",
    "        if predicted == actual:\n",
    "            return 100.0\n",
    "        elif actual == 0:\n",
    "            return 0.0 if predicted != 0 else 100.0\n",
    "        else:\n",
    "            error = abs(predicted - actual) / actual\n",
    "            accuracy = max(0, (1 - error)) * 100\n",
    "            return min(accuracy, 100.0)\n",
    "\n",
    "    def read_log_file(self, file_path: str) -> str:\n",
    "        file_path = Path(file_path)\n",
    "        file_extension = file_path.suffix.lower()\n",
    "\n",
    "        if file_extension in ['.txt', '.log']:\n",
    "            with open(file_path, 'r') as f:\n",
    "                return f.read()\n",
    "        elif file_extension == '.csv':\n",
    "            log_content = []\n",
    "            with open(file_path, 'r') as f:\n",
    "                csv_reader = csv.reader(f)\n",
    "                for row in csv_reader:\n",
    "                    log_content.append(','.join(row))\n",
    "            return '\\n'.join(log_content)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_extension}\")\n",
    "\n",
    "    def evaluate(self, log_file: str, ground_truth_file: str, prompts: List[str], metric_names: List[str]) -> Dict[str, float]:\n",
    "        log_content = self.read_log_file(log_file)\n",
    "\n",
    "        with open(ground_truth_file, 'r') as f:\n",
    "            ground_truth = json.load(f)\n",
    "\n",
    "        results = {}\n",
    "        for prompt, metric_name in zip(prompts, metric_names):\n",
    "            model_response = self.generate_response(prompt, log_content)\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Model response: {model_response}\")\n",
    "            try:\n",
    "                extracted_value = self.extract_numeric_value(model_response)\n",
    "                ground_truth_value = ground_truth[metric_name]\n",
    "                print(f\"Extracted value: {extracted_value}\")\n",
    "                print(f\"Ground truth value: {ground_truth_value}\")\n",
    "                accuracy = self.calculate_accuracy(extracted_value, ground_truth_value)\n",
    "                results[metric_name] = accuracy\n",
    "            except ValueError as e:\n",
    "                print(f\"Error processing {metric_name}: {str(e)}\")\n",
    "                results[metric_name] = 0.0\n",
    "            print(f\"Calculated accuracy: {accuracy}%\\n\")\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How many times did the laptop crash in the past hour?\n",
      "Model response: According to the log, there were two system crashes detected in the past hour:\n",
      "\n",
      "1. At 2024-09-25 11:00:00\n",
      "2. At 2024-09-25 11:59:59\n",
      "\n",
      "So, the laptop crashed twice in the past hour. The answer is 2.\n",
      "Extracted value: 2.0\n",
      "Ground truth value: 2\n",
      "Calculated accuracy: 100.0%\n",
      "\n",
      "Prompt: What was the average CPU usage percentage?\n",
      "Model response: To calculate the average CPU usage, I will add up all the CPU usage values (42% + 38% + 56%) and divide by the total number of values.\n",
      "\n",
      "Average CPU usage = (42 + 38 + 56) / 3\n",
      "= 136 / 3\n",
      "= 45.33%\n",
      "\n",
      "So, the average CPU usage percentage was approximately **45.33%.**\n",
      "Extracted value: 45.33\n",
      "Ground truth value: 45.3\n",
      "Calculated accuracy: 99.93377483443709%\n",
      "\n",
      "Prompt: How many total network requests were made?\n",
      "Model response: To find the total number of network requests, we need to add up the values from each relevant entry in the log. There are three entries:\n",
      "\n",
      "- 45 (2024-09-25 11:15:18)\n",
      "- 38 (2024-09-25 11:30:27)\n",
      "- 37 (2024-09-25 11:45:55)\n",
      "\n",
      "Total network requests = 45 + 38 + 37\n",
      "Total network requests = 120\n",
      "\n",
      "So, a total of **120** network requests were made.\n",
      "Extracted value: 120.0\n",
      "Ground truth value: 120\n",
      "Calculated accuracy: 100.0%\n",
      "\n",
      "Evaluation Results:\n",
      "laptop_crashes: 100.00% accuracy\n",
      "avg_cpu_usage: 99.93% accuracy\n",
      "total_network_requests: 100.00% accuracy\n"
     ]
    }
   ],
   "source": [
    "# from language_model_evaluator import LanguageModelEvaluator\n",
    "def main():\n",
    "    model_name = \"llama3.1\"  # Replace with the model you want to use\n",
    "    log_file = \"sample_log.txt\"\n",
    "    ground_truth_file = \"ground_truth.json\"\n",
    "    prompts = [\n",
    "        \"How many times did the laptop crash in the past hour?\",\n",
    "        \"What was the average CPU usage percentage?\",\n",
    "        \"How many total network requests were made?\"\n",
    "    ]\n",
    "    metric_names = [\"laptop_crashes\", \"avg_cpu_usage\", \"total_network_requests\"]\n",
    "\n",
    "    evaluator = LanguageModelEvaluator(model_name)\n",
    "    results = evaluator.evaluate(log_file, ground_truth_file, prompts, metric_names)\n",
    "\n",
    "    print(\"Evaluation Results:\")\n",
    "    for metric, accuracy in results.items():\n",
    "        print(f\"{metric}: {accuracy:.2f}% accuracy\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LanguageModelEvaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan your explain the first log to me?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow many times did the event with eventid E189 occur?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow many times did the event with the user authorMacBook-Pro occur?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m ]\n\u001b[0;32m     20\u001b[0m metric_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE189\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE188\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE120\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE203\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE323\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcom.apple.cts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorecaptured\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQQ\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMicrosoft Word\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthorMacBook-Pro\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 22\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mLanguageModelEvaluator\u001b[49m(model_name)\n\u001b[0;32m     23\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(log_file, ground_truth_file, prompts, metric_names)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LanguageModelEvaluator' is not defined"
     ]
    }
   ],
   "source": [
    "# from language_model_evaluator import LanguageModelEvaluator\n",
    "def main():\n",
    "    model_name = \"llama3.1\"  # Replace with the model you want to use\n",
    "    log_file = \"Mac_2k.log_structured.csv\"\n",
    "    ground_truth_file = \"mac_ground_truth.json\"\n",
    "    prompts = [\n",
    "        \"Can your explain the first log to me?\",\n",
    "        \"How many times did the event with eventid E189 occur?\",\n",
    "        \"How many times did the event with eventid E188 occur?\",\n",
    "        \"How many times did the event with eventid E120 occur?\",\n",
    "        \"How many times did the event with eventid E203 occur?\",\n",
    "        \"How many times did the event with eventid E323 occur?\",\n",
    "        \"How many times did the event with component kernel occur?\",\n",
    "        \"How many times did the event with component com.apple.cts occur?\",\n",
    "        \"How many times did the event with component corecaptured occur?\",\n",
    "        \"How many times did the event with component QQ occur?\",\n",
    "        \"How many times did the event with component Microsoft Word occur?\",\n",
    "        \"How many times did the event with the user authorMacBook-Pro occur?\",\n",
    "    ]\n",
    "    metric_names = ['test','E189', 'E188', 'E120', 'E203', 'E323', 'kernel', 'com.apple.cts', 'corecaptured', 'QQ', 'Microsoft Word', 'authorMacBook-Pro']\n",
    "\n",
    "    evaluator = LanguageModelEvaluator(model_name)\n",
    "    results = evaluator.evaluate(log_file, ground_truth_file, prompts, metric_names)\n",
    "\n",
    "    print(\"Evaluation Results:\")\n",
    "    for metric, accuracy in results.items():\n",
    "        print(f\"{metric}: {accuracy:.2f}% accuracy\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasai import Agent\n",
    "from regular_agent.agent_ai import Agent_Ai\n",
    "\n",
    "def pandas_legend(self):\n",
    "    llm  = self.get_llm().llm\n",
    "    pandas_ai = Agent(\n",
    "        self.df, # Replace this with the log df\n",
    "        config={\n",
    "        \"llm\":llm,\n",
    "        \"open_charts\":False,\n",
    "        \"enable_cache\" : False,\n",
    "        \"save_charts\": True,\n",
    "        \"max_retries\":3,\n",
    "    })\n",
    "    return pandas_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement regular_agent.agent_ai (from versions: none)\n",
      "ERROR: No matching distribution found for regular_agent.agent_ai\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandasai\n",
    "!pip install regular_agent.agent_ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
