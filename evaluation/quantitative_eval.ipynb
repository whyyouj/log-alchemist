{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Evaluation of model with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasai import Agent\n",
    "\n",
    "def pandas_legend(self):\n",
    "    llm  = self.get_llm().llm\n",
    "    pandas_ai = Agent(\n",
    "        self.df, # Replace this with the log df\n",
    "        config={\n",
    "        \"llm\":llm,\n",
    "        \"open_charts\":False,\n",
    "        \"enable_cache\" : False,\n",
    "        \"save_charts\": True,\n",
    "        \"max_retries\":3,\n",
    "    })\n",
    "    return pandas_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import ollama\n",
    "import csv\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class LanguageModelEvaluator:\n",
    "    def __init__(self, model_name: str, model_type: str = 'ollama'):\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.model = self._load_model()\n",
    "        self.pandas_agent = None\n",
    "\n",
    "    def _load_model(self):\n",
    "        if self.model_type == 'ollama':\n",
    "            return ollama.Client()\n",
    "        elif self.model_type == 'pandasai':\n",
    "            return None\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Model type {self.model_type} not supported\")\n",
    "\n",
    "    def generate_response(self, prompt: str, context: str = None) -> str:\n",
    "        if self.model_type == 'ollama':\n",
    "            response = self.model.chat(model=self.model_name, messages=[\n",
    "                {\n",
    "                    'role': 'system',\n",
    "                    'content': f\"You are an AI assistant analyzing log files. Here's the log content:\\n\\n{context}\\n\\nAnswer the following question based on this log and remember to return a number at the end where applicable:\"\n",
    "                },\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': prompt,\n",
    "                }\n",
    "            ])\n",
    "            return response['message']['content']\n",
    "        elif self.model_type == 'pandasai':\n",
    "            if self.pandas_agent is None:\n",
    "                raise ValueError(\"PandasAI agent is not initialized. Call set_pandas_agent first.\")\n",
    "            return self.pandas_agent.chat(prompt)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Model type {self.model_type} not supported\")\n",
    "\n",
    "    def set_pandas_agent(self, df: pd.DataFrame):\n",
    "        if self.model_type == 'pandasai':\n",
    "            self.pandas_agent = Agent(df, config={\n",
    "                \"open_charts\": False,\n",
    "                \"enable_cache\": False,\n",
    "                \"save_charts\": True,\n",
    "                \"max_retries\": 3,\n",
    "            })\n",
    "        else:\n",
    "            raise ValueError(\"set_pandas_agent is only applicable for 'pandasai' model type\")\n",
    "\n",
    "\n",
    "    def extract_numeric_value(self, text: str) -> float:\n",
    "        # Use a non-capturing group to get full matches\n",
    "        matches = re.findall(r'\\d+(?:\\.\\d+)?', text)\n",
    "        if matches:\n",
    "            # Return the last numeric value found\n",
    "            return float(matches[-1])\n",
    "        else:\n",
    "            raise ValueError(f\"No numeric value found in the text: {text}\")\n",
    "    \n",
    "    def calculate_accuracy(self, predicted: float, actual: float) -> float:\n",
    "        if predicted == actual:\n",
    "            return 100.0\n",
    "        elif actual == 0:\n",
    "            return 0.0 if predicted != 0 else 100.0\n",
    "        else:\n",
    "            error = abs(predicted - actual) / actual\n",
    "            accuracy = max(0, (1 - error)) * 100\n",
    "            return min(accuracy, 100.0)\n",
    "\n",
    "    def read_log_file(self, file_path: str) -> str:\n",
    "        file_path = Path(file_path)\n",
    "        file_extension = file_path.suffix.lower()\n",
    "\n",
    "        if file_extension in ['.txt', '.log']:\n",
    "            with open(file_path, 'r') as f:\n",
    "                return f.read()\n",
    "        elif file_extension == '.csv':\n",
    "            log_content = []\n",
    "            with open(file_path, 'r') as f:\n",
    "                csv_reader = csv.reader(f)\n",
    "                for row in csv_reader:\n",
    "                    log_content.append(','.join(row))\n",
    "            return '\\n'.join(log_content)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_extension}\")\n",
    "\n",
    "    def evaluate(self, log_file: str, ground_truth_file: str, prompts: List[str], metric_names: List[str]) -> Dict[str, float]:\n",
    "        if self.model_type == 'pandasai':\n",
    "            df = pd.read_csv(log_file)\n",
    "            self.set_pandas_agent(df)\n",
    "        else:\n",
    "            log_content = self.read_log_file(log_file)\n",
    "\n",
    "        with open(ground_truth_file, 'r') as f:\n",
    "            ground_truth = json.load(f)\n",
    "\n",
    "        results = {}\n",
    "        for prompt, metric_name in zip(prompts, metric_names):\n",
    "            model_response = self.generate_response(prompt, log_content if self.model_type != 'pandasai' else None)\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Model response: {model_response}\")\n",
    "            try:\n",
    "                extracted_value = self.extract_numeric_value(model_response)\n",
    "                ground_truth_value = ground_truth[metric_name]\n",
    "                print(f\"Extracted value: {extracted_value}\")\n",
    "                print(f\"Ground truth value: {ground_truth_value}\")\n",
    "                accuracy = self.calculate_accuracy(extracted_value, ground_truth_value)\n",
    "                results[metric_name] = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"model_response\": model_response,\n",
    "                    \"extracted_value\": extracted_value,\n",
    "                    \"ground_truth_value\": ground_truth_value,\n",
    "                    \"accuracy\": accuracy\n",
    "                }\n",
    "            except ValueError as e:\n",
    "                print(f\"Error processing {metric_name}: {str(e)}\")\n",
    "                results[metric_name] = {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"model_response\": model_response,\n",
    "                    \"error\": str(e),\n",
    "                    \"accuracy\": 0.0\n",
    "                }\n",
    "            print(f\"Calculated accuracy: {accuracy}%\\n\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_results(self, results: Dict[str, Dict], output_file: str = None):\n",
    "        if output_file is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_file = f\"evaluation_results_{timestamp}.json\"\n",
    "\n",
    "        output = {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"model_type\": self.model_type,\n",
    "            \"evaluation_time\": datetime.now().isoformat(),\n",
    "            \"results\": results\n",
    "        }\n",
    "\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(output, f, indent=2)\n",
    "\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "        return output_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How many times did the laptop crash in the past hour?\n",
      "Model response: According to the logs, there were two system crashes in the past hour:\n",
      "\n",
      "[2024-09-25 11:00:00] System crash detected\n",
      "[2024-09-25 11:59:59] System crash detected\n",
      "\n",
      "So, the answer is 2.\n",
      "Extracted value: 2.0\n",
      "Ground truth value: 2\n",
      "Calculated accuracy: 100.0%\n",
      "\n",
      "Prompt: What was the average CPU usage percentage?\n",
      "Model response: To calculate the average CPU usage, we need to add up all the CPU usage percentages and divide by the total number of readings.\n",
      "\n",
      "Here are the CPU usage percentages:\n",
      "\n",
      "* 42%\n",
      "* 38%\n",
      "* 56%\n",
      "\n",
      "Total CPU usage: 42 + 38 + 56 = 136\n",
      "Number of readings: 3\n",
      "\n",
      "Average CPU usage: 136 / 3 = 45.33%\n",
      "\n",
      "So, the average CPU usage percentage was approximately 45.33%.\n",
      "Extracted value: 45.33\n",
      "Ground truth value: 45.3\n",
      "Calculated accuracy: 99.93377483443709%\n",
      "\n",
      "Prompt: How many total network requests were made?\n",
      "Model response: To find the total number of network requests, I will add up the values reported in the log:\n",
      "\n",
      "45 (at 11:15:18) + 38 (at 11:30:27) + 37 (at 11:45:55)\n",
      "\n",
      "Total = 120\n",
      "\n",
      "So, a total of 120 network requests were made.\n",
      "Extracted value: 120.0\n",
      "Ground truth value: 120\n",
      "Calculated accuracy: 100.0%\n",
      "\n",
      "Results saved to evaluation_results_20241005_203714.json\n",
      "\n",
      "Evaluation Results:\n",
      "laptop_crashes: 100.00% accuracy\n",
      "avg_cpu_usage: 99.93% accuracy\n",
      "total_network_requests: 100.00% accuracy\n",
      "\n",
      "Detailed results have been saved to evaluation_results_20241005_203714.json\n"
     ]
    }
   ],
   "source": [
    "# from language_model_evaluator import LanguageModelEvaluator\n",
    "def main():\n",
    "    model_name = \"llama3.1\" \n",
    "    log_file = \"sample_log.txt\"\n",
    "    ground_truth_file = \"ground_truth.json\"\n",
    "    prompts = [\n",
    "        \"How many times did the laptop crash in the past hour?\",\n",
    "        \"What was the average CPU usage percentage?\",\n",
    "        \"How many total network requests were made?\"\n",
    "    ]\n",
    "    metric_names = [\"laptop_crashes\", \"avg_cpu_usage\", \"total_network_requests\"]\n",
    "\n",
    "    evaluator = LanguageModelEvaluator(model_name)\n",
    "    results = evaluator.evaluate(log_file, ground_truth_file, prompts, metric_names)\n",
    "\n",
    "    output_file = evaluator.save_results(results)\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    for metric, data in results.items():\n",
    "        print(f\"{metric}: {data['accuracy']:.2f}% accuracy\")\n",
    "\n",
    "    print(f\"\\nDetailed results have been saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from language_model_evaluator import LanguageModelEvaluator\n",
    "def main():\n",
    "    model_name = \"pandasai\" \n",
    "    log_file = \"Mac_2k.log_structured.csv\"\n",
    "    ground_truth_file = \"mac_ground_truth.json\"\n",
    "    prompts = [\n",
    "        \"How many times did the event with eventid E189 occur?\",\n",
    "        \"How many times did the event with eventid E188 occur?\",\n",
    "        \"How many times did the event with eventid E120 occur?\",\n",
    "        \"How many times did the event with eventid E203 occur?\",\n",
    "        \"How many times did the event with eventid E323 occur?\",\n",
    "        \"How many times did the event with component kernel occur?\",\n",
    "        \"How many times did the event with component com.apple.cts occur?\",\n",
    "        \"How many times did the event with component corecaptured occur?\",\n",
    "        \"How many times did the event with component QQ occur?\",\n",
    "        \"How many times did the event with component Microsoft Word occur?\",\n",
    "        \"How many times did the event with the user authorMacBook-Pro occur?\",\n",
    "    ]\n",
    "    metric_names = ['E189', 'E188', 'E120', 'E203', 'E323', 'kernel', 'com.apple.cts', 'corecaptured', 'QQ', 'Microsoft Word', 'authorMacBook-Pro']\n",
    "\n",
    "    evaluator = LanguageModelEvaluator(model_name, model_type='pandasai')\n",
    "    results = evaluator.evaluate(log_file, ground_truth_file, prompts, metric_names)\n",
    "\n",
    "    print(\"Evaluation Results:\")\n",
    "    for metric, accuracy in results.items():\n",
    "        print(f\"{metric}: {accuracy:.2f}% accuracy\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandasai import Agent\n",
    "\n",
    "# def pandas_legend(self):\n",
    "#     llm  = self.get_llm().llm\n",
    "#     pandas_ai = Agent(\n",
    "#         self.df, # Replace this with the log df\n",
    "#         config={\n",
    "#         \"llm\":llm,\n",
    "#         \"open_charts\":False,\n",
    "#         \"enable_cache\" : False,\n",
    "#         \"save_charts\": True,\n",
    "#         \"max_retries\":3,\n",
    "#     })\n",
    "#     return pandas_ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
