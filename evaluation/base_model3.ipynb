{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import ollama\n",
    "import csv\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "class LanguageModelEvaluator:\n",
    "    def __init__(self, model_name: str, model_type: str = 'ollama'):\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.model = self._load_model()\n",
    "        # We'll use the same model for evaluation\n",
    "        self.evaluator_model = self.model\n",
    "\n",
    "    def _load_model(self):\n",
    "        if self.model_type == 'ollama':\n",
    "            return ollama.Client()\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Model type {self.model_type} not supported\")\n",
    "\n",
    "    def generate_response(self, prompt: str, context: str = None) -> str:\n",
    "        if self.model_type == 'ollama':\n",
    "            response = self.model.chat(model=self.model_name, messages=[\n",
    "                {\n",
    "                    'role': 'system',\n",
    "                    'content': f\"You are an AI assistant analyzing log files. Here's the log content:\\n\\n{context}\\n\\nAnswer the following question based on this log.\"\n",
    "                },\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': prompt,\n",
    "                }\n",
    "            ])\n",
    "            return response['message']['content']\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Model type {self.model_type} not supported\")\n",
    "\n",
    "    def evaluate_response(self, question: str, response: str, reference_answer: str, rubric: str) -> Dict[str, any]:\n",
    "        # Prepare the evaluation prompt\n",
    "        evaluation_prompt = f\"\"\"\n",
    "You are an expert evaluator. Your task is to evaluate the following response to a question based on the reference answer and the provided rubric. Provide a score and feedback.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "Reference Answer: {reference_answer}\n",
    "\n",
    "Rubric:\n",
    "{rubric}\n",
    "\n",
    "Please provide your evaluation as follows:\n",
    "Score: [1-5]\n",
    "Feedback: [Your detailed feedback]\n",
    "\n",
    "Remember, the score should be an integer between 1 and 5.\n",
    "\n",
    "Evaluation:\n",
    "\"\"\"\n",
    "        # Generate evaluation using Llama 3.1\n",
    "        eval_response = self.evaluator_model.chat(model=self.model_name, messages=[\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': \"You are an expert evaluator.\"\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': evaluation_prompt,\n",
    "            }\n",
    "        ])\n",
    "        eval_text = eval_response['message']['content']\n",
    "\n",
    "        # Extract score and feedback\n",
    "        score_match = re.search(r\"Score:\\s*(\\d+)\", eval_text)\n",
    "        feedback_match = re.search(r\"Feedback:\\s*(.*)\", eval_text, re.DOTALL)\n",
    "\n",
    "        if score_match:\n",
    "            score = int(score_match.group(1))\n",
    "        else:\n",
    "            score = None\n",
    "\n",
    "        if feedback_match:\n",
    "            feedback = feedback_match.group(1).strip()\n",
    "        else:\n",
    "            feedback = None\n",
    "\n",
    "        return {\n",
    "            'score': score,\n",
    "            'feedback': feedback,\n",
    "            'evaluation_text': eval_text\n",
    "        }\n",
    "\n",
    "    def read_log_file(self, file_path: str) -> str:\n",
    "        file_path = Path(file_path)\n",
    "        file_extension = file_path.suffix.lower()\n",
    "\n",
    "        if file_extension in ['.txt', '.log']:\n",
    "            with open(file_path, 'r') as f:\n",
    "                return f.read()\n",
    "        elif file_extension == '.csv':\n",
    "            log_content = []\n",
    "            with open(file_path, 'r') as f:\n",
    "                csv_reader = csv.reader(f)\n",
    "                for row in csv_reader:\n",
    "                    log_content.append(','.join(row))\n",
    "                return '\\n'.join(log_content)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_extension}\")\n",
    "\n",
    "    def evaluate(self, log_file: str, ground_truth_file: str, prompts: List[str], metric_names: List[str]) -> Dict[str, Dict[str, any]]:\n",
    "        log_content = self.read_log_file(log_file)\n",
    "\n",
    "        with open(ground_truth_file, 'r') as f:\n",
    "            ground_truth = json.load(f)\n",
    "\n",
    "        # Define the scoring rubric\n",
    "        score_rubric = \"\"\"\n",
    "Score 1: The response is inaccurate or irrelevant to the question.\n",
    "Score 2: The response has some relevance but contains significant inaccuracies or omissions.\n",
    "Score 3: The response is generally accurate but lacks some important details.\n",
    "Score 4: The response is accurate and includes most of the important details.\n",
    "Score 5: The response is comprehensive, accurate, and addresses all aspects of the question thoroughly.\n",
    "\"\"\"\n",
    "\n",
    "        results = {}\n",
    "        for prompt, metric_name in zip(prompts, metric_names):\n",
    "            model_response = self.generate_response(prompt, log_content)\n",
    "            reference_answer = ground_truth.get(metric_name, \"\")\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Model Response: {model_response}\")\n",
    "            print(f\"Reference Answer: {reference_answer}\")\n",
    "\n",
    "            evaluation = self.evaluate_response(prompt, model_response, reference_answer, score_rubric)\n",
    "            print(f\"Score: {evaluation['score']}\")\n",
    "            print(f\"Feedback: {evaluation['feedback']}\\n\")\n",
    "\n",
    "            results[metric_name] = {\n",
    "                'score': evaluation['score'],\n",
    "                'feedback': evaluation['feedback'],\n",
    "                'model_response': model_response,\n",
    "                'reference_answer': reference_answer\n",
    "            }\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Why did the system restart?\n",
      "Model Response: Based on the log content, it appears that the system restarted due to a \"System crash detected\" at 10:00:01 is not present but there is an entry at 11:00:00 which indicates a possible cause for the restart. However, another System crash detected event occurred before this restart.\n",
      "\n",
      "More specifically, it's likely that the system crashed after the CPU usage spike at 10:45:12 (when CPU usage reached 56%), and then restarted. This is followed by another crash just before midnight.\n",
      "Reference Answer: The system restarted due to a scheduled maintenance update at 02:00 AM.\n",
      "Score: 2\n",
      "Feedback: The response attempts to address the question, but it's marred by significant inaccuracies. The response provides a lengthy analysis of system crashes and CPU usage spikes, which are not relevant to the actual cause of the restart (scheduled maintenance update). Instead of acknowledging that the system restarted due to the scheduled update at 02:00 AM, the response focuses on incorrect events leading up to the restart.\n",
      "\n",
      "While the response shows some attempt to analyze system logs, it fails to align with the correct answer. A score of 2 reflects this significant deviation from the accurate reference answer.\n",
      "\n",
      "Prompt: Why did the system crash?\n",
      "Model Response: Unfortunately, the provided log content does not provide a specific reason for why the system crashed. It only indicates that a system crash was detected at two different times (11:00:00 and 11:59:59), but it does not give any details about what might have caused these crashes.\n",
      "\n",
      "However, we can make some educated guesses based on the log content:\n",
      "\n",
      "1. **High CPU usage**: Before the first crash (at 10:45:12), the CPU usage was at 56%, which is relatively high for a system that started with normal startup behavior. It's possible that prolonged high CPU usage might have contributed to the system crash.\n",
      "2. **Increased network requests**: Around the time of the second crash (11:59:59), the network requests had been steadily decreasing, but it still remains unclear if this was related to the system failure.\n",
      "\n",
      "More detailed log information or specific error logs would be needed to pinpoint an exact cause for the system crashes.\n",
      "Reference Answer: The system crashed because of an unhandled exception caused by a null pointer in the application code.\n",
      "Score: 3\n",
      "Feedback: The response is generally accurate in that it acknowledges the system crash but does not pinpoint the exact cause. However, it makes some educated guesses based on the log content, which are plausible but ultimately incorrect (High CPU usage and Increased network requests). The main issue with this response is that it does not provide a clear explanation for the crashes, despite mentioning that more detailed log information would be needed to determine an exact cause. While it shows some understanding of the system's behavior, it falls short of providing a definitive answer like the reference answer (Unhandled exception caused by a null pointer). To reach a higher score, the response could have been more direct and accurate in its explanation for the crashes.\n",
      "\n",
      "Metric: reason_for_restart\n",
      "Score: 2\n",
      "Feedback: The response attempts to address the question, but it's marred by significant inaccuracies. The response provides a lengthy analysis of system crashes and CPU usage spikes, which are not relevant to the actual cause of the restart (scheduled maintenance update). Instead of acknowledging that the system restarted due to the scheduled update at 02:00 AM, the response focuses on incorrect events leading up to the restart.\n",
      "\n",
      "While the response shows some attempt to analyze system logs, it fails to align with the correct answer. A score of 2 reflects this significant deviation from the accurate reference answer.\n",
      "Model Response: Based on the log content, it appears that the system restarted due to a \"System crash detected\" at 10:00:01 is not present but there is an entry at 11:00:00 which indicates a possible cause for the restart. However, another System crash detected event occurred before this restart.\n",
      "\n",
      "More specifically, it's likely that the system crashed after the CPU usage spike at 10:45:12 (when CPU usage reached 56%), and then restarted. This is followed by another crash just before midnight.\n",
      "Reference Answer: The system restarted due to a scheduled maintenance update at 02:00 AM.\n",
      "\n",
      "Metric: reason_for_crash\n",
      "Score: 3\n",
      "Feedback: The response is generally accurate in that it acknowledges the system crash but does not pinpoint the exact cause. However, it makes some educated guesses based on the log content, which are plausible but ultimately incorrect (High CPU usage and Increased network requests). The main issue with this response is that it does not provide a clear explanation for the crashes, despite mentioning that more detailed log information would be needed to determine an exact cause. While it shows some understanding of the system's behavior, it falls short of providing a definitive answer like the reference answer (Unhandled exception caused by a null pointer). To reach a higher score, the response could have been more direct and accurate in its explanation for the crashes.\n",
      "Model Response: Unfortunately, the provided log content does not provide a specific reason for why the system crashed. It only indicates that a system crash was detected at two different times (11:00:00 and 11:59:59), but it does not give any details about what might have caused these crashes.\n",
      "\n",
      "However, we can make some educated guesses based on the log content:\n",
      "\n",
      "1. **High CPU usage**: Before the first crash (at 10:45:12), the CPU usage was at 56%, which is relatively high for a system that started with normal startup behavior. It's possible that prolonged high CPU usage might have contributed to the system crash.\n",
      "2. **Increased network requests**: Around the time of the second crash (11:59:59), the network requests had been steadily decreasing, but it still remains unclear if this was related to the system failure.\n",
      "\n",
      "More detailed log information or specific error logs would be needed to pinpoint an exact cause for the system crashes.\n",
      "Reference Answer: The system crashed because of an unhandled exception caused by a null pointer in the application code.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    evaluator = LanguageModelEvaluator(model_name='llama3.1')\n",
    "\n",
    "    # Define your prompts and metric names\n",
    "    prompts = [\n",
    "        \"Why did the system restart?\",\n",
    "        \"Why did the system crash?\"\n",
    "    ]\n",
    "\n",
    "    metric_names = [\n",
    "        \"reason_for_restart\",\n",
    "        \"reason_for_crash\"\n",
    "    ]\n",
    "\n",
    "    # Evaluate the model\n",
    "    results = evaluator.evaluate(\n",
    "        log_file='./sample_log.txt',\n",
    "        ground_truth_file='./ground_truth_qualitative.json',\n",
    "        # log_file='./Mac_2k.log_structured.csv',\n",
    "        # ground_truth_file='./ground_truth_qualitative.json',\n",
    "        prompts=prompts,\n",
    "        metric_names=metric_names\n",
    "    )\n",
    "\n",
    "\n",
    "    # Print the evaluation results\n",
    "    for metric, result in results.items():\n",
    "        print(f\"Metric: {metric}\")\n",
    "        print(f\"Score: {result['score']}\")\n",
    "        print(f\"Feedback: {result['feedback']}\")\n",
    "        print(f\"Model Response: {result['model_response']}\")\n",
    "        print(f\"Reference Answer: {result['reference_answer']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maybe use cosine similarity with sentence embedding??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "class LanguageModelEvaluator:\n",
    "    # ... (other methods remain the same)\n",
    "\n",
    "    def evaluate_response_similarity(self, response: str, reference_answer: str) -> float:\n",
    "        # Load a pre-trained sentence transformer model\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')  # You can choose a different model\n",
    "        # Encode the sentences\n",
    "        embeddings = model.encode([response, reference_answer], convert_to_tensor=True)\n",
    "        # Compute cosine similarity\n",
    "        cosine_scores = util.cos_sim(embeddings[0], embeddings[1])\n",
    "        similarity_score = cosine_scores.item()\n",
    "        return similarity_score\n",
    "\n",
    "    def evaluate(self, log_file: str, ground_truth_file: str, prompts: List[str], metric_names: List[str]) -> Dict[str, Dict[str, any]]:\n",
    "        log_content = self.read_log_file(log_file)\n",
    "\n",
    "        with open(ground_truth_file, 'r') as f:\n",
    "            ground_truth = json.load(f)\n",
    "\n",
    "        results = {}\n",
    "        for prompt, metric_name in zip(prompts, metric_names):\n",
    "            model_response = self.generate_response(prompt, log_content)\n",
    "            reference_answer = ground_truth.get(metric_name, \"\")\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Model Response: {model_response}\")\n",
    "            print(f\"Reference Answer: {reference_answer}\")\n",
    "\n",
    "            similarity_score = self.evaluate_response_similarity(model_response, reference_answer)\n",
    "            print(f\"Similarity Score: {similarity_score}\\n\")\n",
    "\n",
    "            results[metric_name] = {\n",
    "                'similarity_score': similarity_score,\n",
    "                'model_response': model_response,\n",
    "                'reference_answer': reference_answer\n",
    "            }\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
