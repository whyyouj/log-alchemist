{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install logparser3\n",
    "%pip install langchain_ollama\n",
    "%pip install langchain_ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "github link to logparser3:  https://github.com/logpai/logparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for different models usage and hyperparameters, can reference ./logparser/{model}/demo.py  in github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ColumnGetter import ColumnGetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ColumnGetter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cg \u001b[38;5;241m=\u001b[39m ColumnGetter()\n\u001b[0;32m      2\u001b[0m cols \u001b[38;5;241m=\u001b[39m cg\u001b[38;5;241m.\u001b[39mget_column(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLlama3.1\u001b[39m\u001b[38;5;124m'\u001b[39m, ColumnGetter\u001b[38;5;241m.\u001b[39msample_log, prompt_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfewshot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ColumnGetter' is not defined"
     ]
    }
   ],
   "source": [
    "cg = ColumnGetter()\n",
    "cols = cg.get_column('Llama3.1', ColumnGetter.sample_log, prompt_method='fewshot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = ''\n",
    "with open('../data/raw/HDFS_2k.log') as f:\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        #if count > 50:\n",
    "        #    break\n",
    "        log += (line + '\\n')\n",
    "        #count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Llama3.1, Prompt Method: fewshot\n",
      "['Date', 'Time', 'User', 'Component/Module', 'Process ID', 'Content']\n"
     ]
    }
   ],
   "source": [
    "cg = ColumnGetter()\n",
    "cols = cg.get_column('Llama3.1', log, prompt_method='fewshot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Date> <Time> <User> <Component_Module> <Process_ID> <Content>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "match = re.search(r\"\\[(.*?)\\]\", cols)\n",
    "\n",
    "if match:\n",
    "    columns_str = match.group(1)\n",
    "    columns = [col.strip().strip('\"').strip(\"'\") for col in columns_str.split(',')]\n",
    "    columns = [re.sub(r'[^A-Za-z]', '_', col) for col in columns]\n",
    "    columns = list(map(lambda x: '<'+x+'>', columns))\n",
    "    formatted_columns = ' '.join(columns)\n",
    "    print(formatted_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logparser.NuLog import LogParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 8302.79it/s]\n",
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\logparser\\NuLog\\NuLog.py:377: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  val_cnt = pd.value_counts(data_token_idx_df.iloc[:, column])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch Step: 1 / 400 Loss: 7.388757 Tokens per Sec: 429.093994\n",
      "Epoch Step: 11 / 400 Loss: 4.216971 Tokens per Sec: 1310.610352\n",
      "Epoch Step: 21 / 400 Loss: 3.509356 Tokens per Sec: 478.900482\n",
      "Epoch Step: 31 / 400 Loss: 3.799990 Tokens per Sec: 1283.212402\n",
      "Epoch Step: 41 / 400 Loss: 3.374608 Tokens per Sec: 1294.758667\n",
      "Epoch Step: 51 / 400 Loss: 3.370907 Tokens per Sec: 1318.594482\n",
      "Epoch Step: 61 / 400 Loss: 3.711313 Tokens per Sec: 1319.991089\n",
      "Epoch Step: 71 / 400 Loss: 3.647737 Tokens per Sec: 1323.007080\n",
      "Epoch Step: 81 / 400 Loss: 3.208989 Tokens per Sec: 1345.303833\n",
      "Epoch Step: 91 / 400 Loss: 3.243276 Tokens per Sec: 1343.796753\n",
      "Epoch Step: 101 / 400 Loss: 3.746201 Tokens per Sec: 1249.364624\n",
      "Epoch Step: 111 / 400 Loss: 2.847046 Tokens per Sec: 1313.963257\n",
      "Epoch Step: 121 / 400 Loss: 2.770893 Tokens per Sec: 1324.738403\n",
      "Epoch Step: 131 / 400 Loss: 2.646085 Tokens per Sec: 1339.860718\n",
      "Epoch Step: 141 / 400 Loss: 2.731105 Tokens per Sec: 1278.823486\n",
      "Epoch Step: 151 / 400 Loss: 2.840174 Tokens per Sec: 1323.071289\n",
      "Epoch Step: 161 / 400 Loss: 2.610830 Tokens per Sec: 1328.827393\n",
      "Epoch Step: 171 / 400 Loss: 2.560784 Tokens per Sec: 1359.367310\n",
      "Epoch Step: 181 / 400 Loss: 2.306926 Tokens per Sec: 1306.572510\n",
      "Epoch Step: 191 / 400 Loss: 2.069445 Tokens per Sec: 1369.360962\n",
      "Epoch Step: 201 / 400 Loss: 2.528702 Tokens per Sec: 1363.049561\n",
      "Epoch Step: 211 / 400 Loss: 2.277195 Tokens per Sec: 1335.967896\n",
      "Epoch Step: 221 / 400 Loss: 2.512732 Tokens per Sec: 1336.551758\n",
      "Epoch Step: 231 / 400 Loss: 2.189995 Tokens per Sec: 1347.125366\n",
      "Epoch Step: 241 / 400 Loss: 2.330609 Tokens per Sec: 1337.232178\n",
      "Epoch Step: 251 / 400 Loss: 2.142066 Tokens per Sec: 1382.089355\n",
      "Epoch Step: 261 / 400 Loss: 2.238819 Tokens per Sec: 1338.913330\n",
      "Epoch Step: 271 / 400 Loss: 2.058743 Tokens per Sec: 343.315704\n",
      "Epoch Step: 281 / 400 Loss: 2.310125 Tokens per Sec: 1270.309814\n",
      "Epoch Step: 291 / 400 Loss: 1.928211 Tokens per Sec: 1276.120117\n",
      "Epoch Step: 301 / 400 Loss: 2.300138 Tokens per Sec: 1299.851929\n",
      "Epoch Step: 311 / 400 Loss: 2.198135 Tokens per Sec: 1330.537720\n",
      "Epoch Step: 321 / 400 Loss: 2.288016 Tokens per Sec: 1345.158569\n",
      "Epoch Step: 331 / 400 Loss: 2.060542 Tokens per Sec: 1343.919434\n",
      "Epoch Step: 341 / 400 Loss: 2.096857 Tokens per Sec: 1359.217651\n",
      "Epoch Step: 351 / 400 Loss: 2.274109 Tokens per Sec: 1334.637817\n",
      "Epoch Step: 361 / 400 Loss: 2.133725 Tokens per Sec: 1303.359619\n",
      "Epoch Step: 371 / 400 Loss: 2.648144 Tokens per Sec: 809.968933\n",
      "Epoch Step: 381 / 400 Loss: 1.795991 Tokens per Sec: 1327.064941\n",
      "Epoch Step: 391 / 400 Loss: 2.302879 Tokens per Sec: 1337.690063\n",
      "Epoch Step: 1 / 400\n",
      "Epoch Step: 11 / 400\n",
      "Epoch Step: 21 / 400\n",
      "Epoch Step: 31 / 400\n",
      "Epoch Step: 41 / 400\n",
      "Epoch Step: 51 / 400\n",
      "Epoch Step: 61 / 400\n",
      "Epoch Step: 71 / 400\n",
      "Epoch Step: 81 / 400\n",
      "Epoch Step: 91 / 400\n",
      "Epoch Step: 101 / 400\n",
      "Epoch Step: 111 / 400\n",
      "Epoch Step: 121 / 400\n",
      "Epoch Step: 131 / 400\n",
      "Epoch Step: 141 / 400\n",
      "Epoch Step: 151 / 400\n",
      "Epoch Step: 161 / 400\n",
      "Epoch Step: 171 / 400\n",
      "Epoch Step: 181 / 400\n",
      "Epoch Step: 191 / 400\n",
      "Epoch Step: 201 / 400\n",
      "Epoch Step: 211 / 400\n",
      "Epoch Step: 221 / 400\n",
      "Epoch Step: 231 / 400\n",
      "Epoch Step: 241 / 400\n",
      "Epoch Step: 251 / 400\n",
      "Epoch Step: 261 / 400\n",
      "Epoch Step: 271 / 400\n",
      "Epoch Step: 281 / 400\n",
      "Epoch Step: 291 / 400\n",
      "Epoch Step: 301 / 400\n",
      "Epoch Step: 311 / 400\n",
      "Epoch Step: 321 / 400\n",
      "Epoch Step: 331 / 400\n",
      "Epoch Step: 341 / 400\n",
      "Epoch Step: 351 / 400\n",
      "Epoch Step: 361 / 400\n",
      "Epoch Step: 371 / 400\n",
      "Epoch Step: 381 / 400\n",
      "Epoch Step: 391 / 400\n",
      "Parsing done. [Time taken: 0:00:50.056499]\n"
     ]
    }
   ],
   "source": [
    "input_dir = '../data/raw/' # The input directory of log file\n",
    "output_dir = 'result/'  # The output directory of parsing results\n",
    "\n",
    "\n",
    "log_file = 'HDFS_2k.log'  # The input log file name\n",
    "log_format = '<Date> <Time> <Pid> <Level> <Component> <Content>' # Define log format to split message fields\n",
    "# Regular expression list for optional preprocessing (default: [])\n",
    "regex = [\n",
    "    r'blk_(|-)[0-9]+', # block id\n",
    "    r'(/|)([0-9]+\\.){3}[0-9]+(:[0-9]+|)(:|)', # IP\n",
    "    r'(?<=[^A-Za-z0-9])(\\-?\\+?\\d+)(?=[^A-Za-z0-9])|[0-9]+$', # Numbers\n",
    "]\n",
    "\n",
    "filters = \"(\\s+blk_)|(:)|(\\s)\"\n",
    "k = 15\n",
    "nr_epochs = 1 # Number of epochs to run\n",
    "num_samples = 0\n",
    "\n",
    "parser = LogParser(log_format=log_format, indir=input_dir, outdir=output_dir, filters=filters, k=k)\n",
    "parser.parse(log_file, nr_epochs=nr_epochs, num_samples=num_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
